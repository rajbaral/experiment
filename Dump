Perfect — here’s the enhanced Angular solution for your scan upload flow with:
	1.	Progress bar for each scanned file
	2.	Disable the Upload button during upload
	3.	Retry logic for failed chunks (with 3 attempts max)

⸻

Step-by-Step Enhanced Implementation

⸻

1. Extend ScanUploadService with Progress + Retry

@Injectable({ providedIn: 'root' })
export class ScanUploadService {
  constructor(private http: HttpClient) {}

  async uploadPdfInChunksWithProgress(
    base64: string,
    fileName: string,
    onProgress: (percent: number) => void
  ): Promise<void> {
    const blob = this.base64ToBlob(base64, 'application/pdf');
    const chunks = this.splitBlob(blob, 2 * 1024 * 1024);
    const totalChunks = chunks.length;
    const fileId = fileName;

    let uploaded = 0;

    for (let i = 0; i < totalChunks; i++) {
      await this.uploadChunkWithRetry(chunks[i], fileId, i, totalChunks);
      uploaded++;
      onProgress(Math.floor((uploaded / totalChunks) * 100));
    }
  }

  private async uploadChunkWithRetry(chunk: Blob, fileId: string, chunkIndex: number, totalChunks: number) {
    const formData = new FormData();
    formData.append('fileId', fileId);
    formData.append('chunkIndex', chunkIndex.toString());
    formData.append('totalChunks', totalChunks.toString());
    formData.append('chunk', chunk, `${fileId}_chunk${chunkIndex}.part`);

    let attempts = 0;
    const maxRetries = 3;

    while (attempts < maxRetries) {
      try {
        await this.http.post('/api/upload-chunk', formData).toPromise();
        return;
      } catch (err) {
        attempts++;
        if (attempts >= maxRetries) {
          throw new Error(`Chunk ${chunkIndex} failed after ${maxRetries} retries.`);
        }
        await new Promise(res => setTimeout(res, 1000)); // wait before retry
      }
    }
  }

  private base64ToBlob(base64: string, mime: string): Blob {
    const byteCharacters = atob(base64);
    const byteArrays = [];

    for (let offset = 0; offset < byteCharacters.length; offset += 512) {
      const slice = byteCharacters.slice(offset, offset + 512);
      const byteNumbers = Array.from(slice).map(char => char.charCodeAt(0));
      byteArrays.push(new Uint8Array(byteNumbers));
    }

    return new Blob(byteArrays, { type: mime });
  }

  private splitBlob(blob: Blob, chunkSize: number): Blob[] {
    const chunks: Blob[] = [];
    let offset = 0;

    while (offset < blob.size) {
      const end = Math.min(offset + chunkSize, blob.size);
      chunks.push(blob.slice(offset, end));
      offset = end;
    }

    return chunks;
  }
}


⸻

2. Update ScanDocumentComponent for Upload Logic

Add UI state variables:

isUploading = false;
uploadProgressMap: { [fileName: string]: number } = {};

Update your onUploadScans() method:

async onUploadScans(): Promise<void> {
  if (!this.scannedDocuments.length) return;

  this.isUploading = true;

  for (let doc of this.scannedDocuments) {
    const fileName = doc.name;
    const base64Data = doc.url.replace('data:application/pdf;base64,', '');
    this.uploadProgressMap[fileName] = 0;

    try {
      await this.scanUploadService.uploadPdfInChunksWithProgress(
        base64Data,
        fileName,
        (progress: number) => {
          this.uploadProgressMap[fileName] = progress;
        }
      );
    } catch (err) {
      console.error(`Upload failed for ${fileName}:`, err);
      alert(`Upload failed for ${fileName}: ${err.message}`);
    }
  }

  this.isUploading = false;
  alert('All uploads complete.');
}


⸻

3. Update the HTML Template

Disable button and show progress:

<button [disabled]="isUploading" (click)="onUploadScans()">Upload Scans</button>

<div *ngFor="let doc of scannedDocuments">
  <div>{{ doc.name }}</div>
  <progress [value]="uploadProgressMap[doc.name] || 0" max="100"></progress>
</div>


⸻

All Set

Now you have:
	•	Upload progress per file
	•	Retry logic per chunk (3 attempts)
	•	Button disabled while uploading



Great question — here’s the clear and direct answer:

⸻

Does Asprise Scanner.js provide chunking functionality?

No, Scanner.js does not handle chunking or uploading large files out-of-the-box.

It:
	•	Handles device communication (TWAIN/WIA),
	•	Provides UI for preview/editing scanned pages,
	•	Returns output as Base64, Blob, or file download (PDF/JPEG).

⸻

Why we’re handling chunking manually?

Because:
	1.	Browser upload limits (especially with large PDFs/images) — uploads can fail due to timeout, size, or memory.
	2.	Reliability — chunking with retry lets you resume part of an upload instead of failing the whole file.
	3.	Scalability — Azure Blob Storage (and most cloud providers) recommends chunked/multipart uploads for files over ~4MB.

⸻

Best Practice Summary
	•	Scanner.js: Great for capture + preview
	•	Angular (frontend): Responsible for:
	•	Handling large files
	•	Splitting into chunks
	•	Retry + upload control
	•	Backend: Responsible for assembling chunks + pushing to storage

This decoupled pattern is standard practice in modern apps (used by Dropbox, Google Drive, Azure Storage SDKs, etc.)

⸻

Let me know if you want to compare this with Azure SDK’s own chunk upload (e.g., direct-to-blob via BlockBlobClient) — that’s another option if you’re using SAS tokens.



async onUploadScans(): Promise<void> {
  if (!this.scannedDocuments.length) return;

  this.isUploading = true;
  let allSuccessful = true;

  for (let doc of this.scannedDocuments) {
    const fileName = doc.name;
    const base64Data = doc.url.replace('data:application/pdf;base64,', '');
    this.uploadProgressMap[fileName] = 0;

    try {
      await this.scanUploadService.uploadPdfInChunksWithProgress(
        base64Data,
        fileName,
        (progress: number) => {
          this.uploadProgressMap[fileName] = progress;
        }
      );
    } catch (err) {
      allSuccessful = false;
      console.error(`Upload failed for ${fileName}:`, err);
      alert(`Upload failed for ${fileName}: ${err.message}`);
    }
  }

  this.isUploading = false;

  if (allSuccessful) {
    alert('All uploads complete.');
  } else {
    alert('Some documents failed to upload. Please check the console or try again.');
  }
}
